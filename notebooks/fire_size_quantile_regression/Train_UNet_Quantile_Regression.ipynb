{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG--pk0s_dKN"
      },
      "source": [
        "# Wildfire Forecasting: Quantile Regression on Fire Size\n",
        "\n",
        "This notebook can be used for training a regression model for forecasting wildfire size in hectares. It is intended to be a step-by-step tutorial. We are using the 1 degree version of the [SeasFire Datacube](https://zenodo.org/record/7108392). You can also use the 0.25 degree version, but you might get out of memory errors. The datacube has to be downloaded first. It has many different variables that are all described on the zenodo page. For simplicity sake, we use only a subset. \n",
        "\n",
        "This tutorial is intended to be run on Google Colab with the data stored in Google Drive. You might need to adjust some paths etc. in order to run it locally.\n",
        "\n",
        "ðŸ”¥ \n",
        "**Here you can define which variables from the datacube you want and which datacube you want, into how many quantiles you want to divide the fire sizes. You can also choose the target variable and if you want to train with unmasked or masked loss.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taqdJXW_R0an"
      },
      "outputs": [],
      "source": [
        "NO_QUANTILES = 25\n",
        "DEG_DATACUBE = 1\n",
        "LOSS = \"masked\"\n",
        "DELTA_TIME = 2\n",
        "VARIABLES_SELECTED = ['rel_hum', 'ndvi', 'pop_dens_LOG',\n",
        "                      'tp_LOG', 'sst', 'gwis_ba', 'lst_day']\n",
        "TARGET = 'gwis_ba'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "NQFETpvr8y0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from utils.plot import visualize_batch_prediction\n",
        "# from utils.dataloader import create_datasets_model\n",
        "# from utils.general import seed_everything\n",
        "# from utils.model import UNet"
      ],
      "metadata": {
        "id": "0rUcRv2adXjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "print(f\"GPU available? {train_on_gpu}\")\n",
        "SEED = 172\n",
        "MODEL_NO = 0 # in K-fold\n",
        "N_FOLDS = 10 # in K-fold\n",
        "seed_everything(SEED)"
      ],
      "metadata": {
        "id": "yrS4QFy9mvvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJGT65OL_dKN"
      },
      "outputs": [],
      "source": [
        "%%capture \n",
        "import os\n",
        "import shutil\n",
        "import gc\n",
        "import cv2\n",
        "import json \n",
        "import time\n",
        "import tqdm\n",
        "import random\n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm as tq\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tqdm.notebook import tqdm as ntqdm\n",
        "\n",
        "from typing import List\n",
        "import scipy\n",
        "from scipy.stats import boxcox\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "\n",
        "import albumentations as albu\n",
        "\n",
        "plt.style.use('bmh')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg4ph1zo9QzP"
      },
      "source": [
        "## Load data set\n",
        "\n",
        "First, we need to load the dataset and perform some calculations for normalising the data. We also calculate the quantiles here.\n",
        "\n",
        "ðŸ”¥ **You might need to adjust some paths to reflect your file structure here!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmLqacBBHg5E"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install zarr\n",
        "!pip install shapely cartopy --no-binary shapely --no-binary cartopy\n",
        "!pip install --upgrade geopandas pyshp shapely\n",
        "\n",
        "\n",
        "import geopandas as gpd\n",
        "import xarray as xr\n",
        "from pathlib import Path\n",
        "\n",
        "if DEG_DATACUBE == 1:\n",
        "  ds = xr.open_zarr('/content/drive/MyDrive/seasfire_1deg.zarr') #edit to your path\n",
        "else:\n",
        "  ds = xr.open_zarr('/content/drive/MyDrive/seasfire.zarr').     #edit to your path\n",
        "mask = xr.where(ds.lsm>=0.9,1,0).astype(bool)#\n",
        "\n",
        "\n",
        "# put the log for population_density and total_precipitation (there are too many 0 entries)\n",
        "ds = ds.assign(pop_dens_LOG = lambda x: np.log(1+x['pop_dens']))\n",
        "ds = ds.assign(tp_LOG = lambda x: np.log(1+x['tp']))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data used has very different distributions (e.g. temperatures are given in Kelvin, but the Normalized Difference Vegetation Index is only defined in the range (-1, 1). In order to make it easier for the model to learn, we normalise all data to have a mean of 0, and a standard deviation of 1. The normalisation itself is done in the dataloader, but we calculate the means and standard deviations here.\n",
        "\n",
        "ðŸ”¥ **You might need to adjust some paths to reflect your file structure here!**"
      ],
      "metadata": {
        "id": "AOcJgBp47-kP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0cXeNMgdXKh"
      },
      "outputs": [],
      "source": [
        "file_mean_std = f'/content/drive/MyDrive/challenge - NOA - XAI for Wildfire Forecasting/Notebooks/develop_Johanna/table_mean_std_{DEG_DATACUBE}deg_{\"_\".join(VARIABLES_SELECTED)}.csv'\n",
        "\n",
        "if not Path(file_mean_std).exists():\n",
        "  ### SAVE IN A TABLE GLOBAL MEAN AND STD for EACH VARIABLE\n",
        "  vmean = ds.mean()[VARIABLES_SELECTED].load().to_pandas()\n",
        "  vmean.name = 'mean'\n",
        "  vstd  = ds.std() [VARIABLES_SELECTED].load().to_pandas()\n",
        "  vstd.name  = 'std'\n",
        "  table_mean_std = pd.concat([vmean, vstd], axis=1)\n",
        "  table_mean_std.to_csv(file_mean_std)\n",
        "else:\n",
        "  table_mean_std = pd.read_csv(file_mean_std, index_col = 0, header = 0)\n",
        "\n",
        "table_mean_std"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The raw values of the burned area in hectares have a quite large range from `0` to almost `1,000,000` hecatares in a `1 degree x 1 degree` pixel. We calculate the quantiles of fire sizes excluding 0 values and NaNs in the sea surface (otherwise, the data is too skewed).\n",
        "\n",
        "ðŸ”¥ **You might need to adjust some paths to reflect your file structure here!**\n",
        "ðŸ”¥"
      ],
      "metadata": {
        "id": "a8h0l5bF8Mzz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfL-NPV8YAEY"
      },
      "outputs": [],
      "source": [
        "file_fire_quantiles = f'/content/drive/MyDrive/challenge - NOA - XAI for Wildfire Forecasting/Notebooks/develop_Johanna/fire_quantiles_{DEG_DATACUBE}deg_{NO_QUANTILES}.csv'\n",
        "quantiles = np.linspace(0, 1, num=NO_QUANTILES)\n",
        "if not Path(file_fire_quantiles).exists():\n",
        "  gwis_array = np.asarray(ds.gwis_ba)\n",
        "  quantiles_gwis = np.nanquantile(gwis_array[gwis_array>0.0], quantiles)\n",
        "  fcci_array = np.asarray(ds.fcci_ba)\n",
        "  quantiles_fcci = np.nanquantile(fcci_array[fcci_array>0.0], quantiles)\n",
        "\n",
        "  fire_quantiles = pd.DataFrame([list(quantiles_fcci), list(quantiles_gwis)], [\"fcci_ba\", \"gwis_ba\"], [quantiles])\n",
        "  fire_quantiles.to_csv(file_fire_quantiles)\n",
        "\n",
        "else:\n",
        "  fire_quantiles = pd.read_csv(file_fire_quantiles, index_col = 0, header = 0)\n",
        "\n",
        "fire_quantiles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzArHXHbh2UF"
      },
      "source": [
        "Build a custom Dataset Class (following [PyTorch advices](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files))\n",
        "\n",
        "ðŸ”¥ **Here the quantiles are assigned to the target in __getitem__() (if self.fire_quantiles is not None)**\n",
        "\n",
        "What I am trying to do is this, imagine if these are the burned area values that we have in the whole dataset:\n",
        "\n",
        "1.   I start from the raw burned area values (without NaNs and 0s), e.g. **[1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10]**\n",
        "2.   I want e.g. 5 quantiles  so the **0, 0.25, 0.5, 0.75 and 1** quantiles!\n",
        "3. I calculate the values for the quantiles: **1, 1.75, 3, 6.25, 10**\n",
        "\n",
        "In the **itemgetter** function I then change the burned areas to the enumerated quantile they are in (NaNs are turned into -1 and 0 stay 0) So e.g.:\n",
        "\n",
        "`[[0, 0, NaN], [1, 1, 2], [5, 6, 8]]` would be turned into:\n",
        "`[[0, 0, -1], [0.25, 0.25, 0.5], [0.75, 1, 1]]`\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = ds.chunk(dict(time = len(ds.time),\n",
        "                   latitude = 90, \n",
        "                   longitude = 90))\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset = create_datasets_model(ds[VARIABLES_SELECTED], \n",
        "                                                                   slice_train = slice('20030101', '20161231'),\n",
        "                                                                   slice_valid = slice('20170101', '20181231'), \n",
        "                                                                   slice_test  = None,\n",
        "                                                                   table_mean_std = table_mean_std,\n",
        "                                                                   fire_quantiles = fire_quantiles,\n",
        "                                                                   target='gwis_ba', delta_time = DELTA_TIME, inland_map = mask)"
      ],
      "metadata": {
        "id": "o8RGXf0YgO4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjBiMPPz_dKR"
      },
      "source": [
        "## Model Definition\n",
        "\n",
        "U-Net was originally invented and first used for biomedical image segmentation (here the [original paper](https://arxiv.org/pdf/1505.04597.pdf)). Its architecture can be broadly thought of as an encoder network (contraction block), a bottleneck, followed by a decoder network (expansion section).\n",
        "\n",
        "I you wish to deep dive into U-Net and Image Segmentation, here a [detailed blog post](https://www.jeremyjordan.me/semantic-segmentation/)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet(n_channels=len(train_dataset.features), \n",
        "             n_classes=1,\n",
        "             regression=True).float()\n",
        "             \n",
        "if train_on_gpu:\n",
        "    model.cuda()"
      ],
      "metadata": {
        "id": "KR4W1sNUmR0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJa_Fr1_idqX"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn85ZApG_dKR"
      },
      "outputs": [],
      "source": [
        "num_workers = 0\n",
        "bs = 4\n",
        "\n",
        "# DataLoader to batch the images\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=bs, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    valid_dataset, batch_size=bs, shuffle=False, num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWcLVUoswX93"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary\n",
        "from torch.nn import MSELoss\n",
        "import warnings\n",
        "\n",
        "#ignore some deprecation warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
        "\n",
        "model = UNet(n_channels=len(train_dataset.features), n_classes=1, regression=True).float()\n",
        "if train_on_gpu:\n",
        "    model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_list, valid_loss_list, dice_score_list, lr_rate_list, valid_loss_min = model.train_model(\n",
        "                                                                                          train_loader = train_loader,\n",
        "                                                                                          valid_loader = valid_loader,\n",
        "                                                                                          n_epochs = 32,\n",
        "                                                                                          t_mask = None,\n",
        "                                                                                          criterion = MSELoss(), #Dice is used for segmentation\n",
        "                                                                                          optimizer = optim.Adam(model.parameters(), lr = 0.005),\n",
        "                                                                                          scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.2, patience=2, cooldown=2)\n",
        "                                                                                      )"
      ],
      "metadata": {
        "id": "Ei3Nvj8flB5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(train_loss_list,  marker='o', label=\"Training Loss\")\n",
        "plt.plot(valid_loss_list,  marker='o', label=\"Validation Loss\")\n",
        "plt.ylabel('loss', fontsize=22)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TaYwFQHIm2Gc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "PjBiMPPz_dKR"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}